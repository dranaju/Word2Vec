{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Import the necessary methods from tweepy library\n",
    "from tweepy.streaming import StreamListener\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy import Stream\n",
    "\n",
    "#Variables that contains the user credentials to access Twitter API \n",
    "access_token = \"1013974544862900224-OPAmsjG1LRZXTBvAoEA0vohqfnyNIN\"\n",
    "access_token_secret = \"wy4JzyMbjJXpMmTpf8n7xIc8vM2hwsxI8682nthrpdfeV\"\n",
    "consumer_key = \"b64HcVTXO8gEPQWC98Amool2m\"\n",
    "consumer_secret = \"LivflElnpaAqeZzXdwB5Zqk5elIufBT4k9Zaohp8LtHoDYUtGK\"\n",
    "\n",
    "\n",
    "#This is a basic listener that just prints received tweets to stdout.\n",
    "class StdOutListener(StreamListener):\n",
    "\n",
    "    def on_data(self, data):\n",
    "        print(data)\n",
    "        print(type(data))\n",
    "        return True\n",
    "\n",
    "    def on_error(self, status):\n",
    "        print(status)\n",
    "\n",
    "\n",
    "if (__name__ == '__main__'):\n",
    "\n",
    "    #This handles Twitter authetification and the connection to Twitter Streaming API\n",
    "    l = StdOutListener()\n",
    "    auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_token, access_token_secret)\n",
    "    stream = Stream(auth, l)\n",
    "\n",
    "    #This line filter Twitter Streams to capture data by the keywords: 'python', 'javascript', 'ruby'\n",
    "#     stream.filter(track=['python', 'javascript', 'ruby'])\n",
    "#     while True:\n",
    "#         try:\n",
    "#             stream.filter(locations=[-125,25,-65,48], async=False)##These coordinates are approximate bounding box around USA\n",
    "#             break\n",
    "#         except Exception, e:\n",
    "#              # Abnormal exit: Reconnect\n",
    "#              nsecs=random.randint(60,63)\n",
    "#              time.sleep(nsecs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_data_path = 'twitter1.txt'\n",
    "\n",
    "tweets_data = []\n",
    "tweets_file = open(tweets_data_path, \"r\")\n",
    "for line in tweets_file:\n",
    "    try:\n",
    "        tweet = json.loads(line)\n",
    "        tweets_data.append(tweet)\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n"
     ]
    }
   ],
   "source": [
    "print len(tweets_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'RT @0liveiraaa: @httpsvogue @margarida_velez O \\xe1rbitro a seguir foi mudar os boxers',\n",
       " u'hoje definitivamente n\\xe3o foi meu dia:\\n- esqueci o cartao, voltei pra casa correndo pra nao perder o onibus e perdi\\u2026 https://t.co/SkPiNv01MJ',\n",
       " u'Dia 13 de julho tem Arrai\\xe1 Dos Lazaros, com BARRACA DO BEIJO, TEQUILEIRA e etc!! para mais informa\\xe7\\xf5es veja no even\\u2026 https://t.co/wG7q1PuenS',\n",
       " u'RT @HipHop__Tuga: Fofa, viraste-me as costas e eu caguei,\\npodes ter 40 homens ningu\\xe9m d\\xe1 o que eu te dei https://t.co/MCYMRKwLw7',\n",
       " u'ser pobre eh tudo de ruim',\n",
       " u'RT @luliphaus: Queria estar assim de estudar https://t.co/hls8dZUZt6',\n",
       " u'RT @enpobre: \"se sua casa pegasse fogo e vc tivesse q salvar apenas uma coisa, oq seria?\"\\n\\neu: https://t.co/V58fuJXR2z',\n",
       " u'Gente eu n\\xe3o sei colar c\\xedlios posti\\xe7os n\\xe3o sei n\\xe3o sei parece q meu olho n\\xe3o tem o formato certo',\n",
       " u'RT @divadsite: namoral eu sou APAIXONADA a p a i x o n a d a ApAiXoNaDa apaixonada por mensagem inesperada',\n",
       " u'RT @horansaffair: mano se voc\\xeas escutarem uma mulher gritando por socorro VOC\\xcaS AJUDAM PORRA sendo briga de casal ou n\\xe3o, n importa, uma si\\u2026',\n",
       " u'RT @G_Boyfriend_Br: [Atualiza\\xe7\\xe3o] Os meninos foram \\xe0 um evento para se encontrar com f\\xe3s na Tail\\xe2ndia, dia 01/07, de acordo com a data ocid\\u2026',\n",
       " u'Disse tudo https://t.co/K8CNyLLJBY',\n",
       " u'Eu odeio quando a gente pede pra eu fazer um favor pow odeio odeio',\n",
       " u'@giopands At\\xe9 a mais ruim das pizzas \\xe9 kkk',\n",
       " u'Fico at\\xe9 com vergonha alheia quando ele ta no ch\\xe3o fazendo escandalo #RespeitemMaitePerroni https://t.co/VBzCSO62lX',\n",
       " u'minhas f\\xe9rias ser\\xe3o entediantes',\n",
       " u'RT @cmiIanbhd: namoral se o brasil ganhar sexta os gringos v\\xe3o at\\xe9 querer desativar porque a gente vai ficar NOJENTO',\n",
       " u'RT @dudsac_: @seadupim Bom eu sempre demonstrei pelos outros, at\\xe9 nas suas pequenas conquistas j\\xe1 eu prefiro nem comentar',\n",
       " u'@ros3coloredgirl Eles falam que n\\xe3o n\\xe9, mas cada um tem uma resposta diferente quando perguntam sobre McFly uns fic\\u2026 https://t.co/rMIh0JDfNi',\n",
       " u'RT @UmFilosofoCitou: Um ano atr\\xe1s, eu nunca imaginei que minha vida seria como \\xe9 agora.',\n",
       " u'RT @sgpiotto: Foda de beijar por beijar \\xe9 q sempre vai ter aquela espec\\xedfica q vc queria pra porra',\n",
       " u'@DamaOfficer Ss, vou pensar... Eu ainda n\\xe3o assisti a terra prometida, \\xe9 a minha primeira vez... T\\xf4 pensando em faz\\u2026 https://t.co/26rn5Yrznj']"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map(lambda tweet: tweet['text'], tweets_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'@DamaOfficer Ss, vou pensar... Eu ainda n\\xe3o assisti a terra prometida, \\xe9 a minha primeira vez... T\\xf4 pensando em faz\\u2026 https://t.co/26rn5Yrznj'"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tweets['text'] = map(lambda tweet: tweet['text'], tweets_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     RT @0liveiraaa: @httpsvogue @margarida_velez O...\n",
       "1     hoje definitivamente não foi meu dia:\\n- esque...\n",
       "2     Dia 13 de julho tem Arraiá Dos Lazaros, com BA...\n",
       "3     RT @HipHop__Tuga: Fofa, viraste-me as costas e...\n",
       "4                             ser pobre eh tudo de ruim\n",
       "5     RT @luliphaus: Queria estar assim de estudar h...\n",
       "6     RT @enpobre: \"se sua casa pegasse fogo e vc ti...\n",
       "7     Gente eu não sei colar cílios postiços não sei...\n",
       "8     RT @divadsite: namoral eu sou APAIXONADA a p a...\n",
       "9     RT @horansaffair: mano se vocês escutarem uma ...\n",
       "10    RT @G_Boyfriend_Br: [Atualização] Os meninos f...\n",
       "11                   Disse tudo https://t.co/K8CNyLLJBY\n",
       "12    Eu odeio quando a gente pede pra eu fazer um f...\n",
       "13           @giopands Até a mais ruim das pizzas é kkk\n",
       "14    Fico até com vergonha alheia quando ele ta no ...\n",
       "15                      minhas férias serão entediantes\n",
       "16    RT @cmiIanbhd: namoral se o brasil ganhar sext...\n",
       "17    RT @dudsac_: @seadupim Bom eu sempre demonstre...\n",
       "18    @ros3coloredgirl Eles falam que não né, mas ca...\n",
       "19    RT @UmFilosofoCitou: Um ano atrás, eu nunca im...\n",
       "20    RT @sgpiotto: Foda de beijar por beijar é q se...\n",
       "21    @DamaOfficer Ss, vou pensar... Eu ainda não as...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim, logging\n",
    "import numpy as  np\n",
    "import glob\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO) #configura o gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n",
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "print len(tweets['text'])\n",
    "data = tweets['text']\n",
    "print type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 \to\n",
      "2 \tárbitro\n",
      "3 \ta\n",
      "4 \tseguir\n",
      "5 \tfoi\n",
      "6 \tmudar\n",
      "7 \tos\n",
      "8 \tboxers\n",
      "9 \thoje\n",
      "10 \tdefinitivamente\n",
      "11 \tnão\n",
      "12 \tfoi\n",
      "13 \tmeu\n",
      "14 \tdia\n",
      "15 \tesqueci\n",
      "16 \to\n",
      "17 \tcartao\n",
      "18 \tvoltei\n",
      "19 \tpra\n",
      "20 \tcasa\n",
      "21 \tcorrendo\n",
      "22 \tpra\n",
      "23 \tnao\n",
      "24 \tperder\n",
      "25 \to\n",
      "26 \tonibus\n",
      "27 \te\n",
      "28 \tperdi\n",
      "29 \tdia\n",
      "30 \t13\n",
      "31 \tde\n",
      "32 \tjulho\n",
      "33 \ttem\n",
      "34 \tarraiá\n",
      "35 \tdos\n",
      "36 \tlazaros\n",
      "37 \tcom\n",
      "38 \tbarraca\n",
      "39 \tdo\n",
      "40 \tbeijo\n",
      "41 \ttequileira\n",
      "42 \te\n",
      "43 \tetc\n",
      "44 \tpara\n",
      "45 \tmais\n",
      "46 \tinformações\n",
      "47 \tveja\n",
      "48 \tno\n",
      "49 \teven\n",
      "50 \tfofa\n",
      "51 \tvirasteme\n",
      "52 \tas\n",
      "53 \tcostas\n",
      "54 \te\n",
      "55 \teu\n",
      "56 \tcagueipodes\n",
      "57 \tter\n",
      "58 \t40\n",
      "59 \thomens\n",
      "60 \tninguém\n",
      "61 \tdá\n",
      "62 \to\n",
      "63 \tque\n",
      "64 \teu\n",
      "65 \tte\n",
      "66 \tdei\n",
      "67 \tser\n",
      "68 \tpobre\n",
      "69 \teh\n",
      "70 \ttudo\n",
      "71 \tde\n",
      "72 \truim\n",
      "73 \tqueria\n",
      "74 \testar\n",
      "75 \tassim\n",
      "76 \tde\n",
      "77 \testudar\n",
      "78 \tse\n",
      "79 \tsua\n",
      "80 \tcasa\n",
      "81 \tpegasse\n",
      "82 \tfogo\n",
      "83 \te\n",
      "84 \tvc\n",
      "85 \ttivesse\n",
      "86 \tq\n",
      "87 \tsalvar\n",
      "88 \tapenas\n",
      "89 \tuma\n",
      "90 \tcoisa\n",
      "91 \toq\n",
      "92 \tseriaeu\n",
      "93 \tgente\n",
      "94 \teu\n",
      "95 \tnão\n",
      "96 \tsei\n",
      "97 \tcolar\n",
      "98 \tcílios\n",
      "99 \tpostiços\n",
      "100 \tnão\n",
      "101 \tsei\n",
      "102 \tnão\n",
      "103 \tsei\n",
      "104 \tparece\n",
      "105 \tq\n",
      "106 \tmeu\n",
      "107 \tolho\n",
      "108 \tnão\n",
      "109 \ttem\n",
      "110 \to\n",
      "111 \tformato\n",
      "112 \tcerto\n",
      "113 \tnamoral\n",
      "114 \teu\n",
      "115 \tsou\n",
      "116 \tapaixonada\n",
      "117 \ta\n",
      "118 \tp\n",
      "119 \ta\n",
      "120 \ti\n",
      "121 \tx\n",
      "122 \to\n",
      "123 \tn\n",
      "124 \ta\n",
      "125 \td\n",
      "126 \ta\n",
      "127 \tapaixonada\n",
      "128 \tapaixonada\n",
      "129 \tpor\n",
      "130 \tmensagem\n",
      "131 \tinesperada\n",
      "132 \tmano\n",
      "133 \tse\n",
      "134 \tvocês\n",
      "135 \tescutarem\n",
      "136 \tuma\n",
      "137 \tmulher\n",
      "138 \tgritando\n",
      "139 \tpor\n",
      "140 \tsocorro\n",
      "141 \tvocês\n",
      "142 \tajudam\n",
      "143 \tporra\n",
      "144 \tsendo\n",
      "145 \tbriga\n",
      "146 \tde\n",
      "147 \tcasal\n",
      "148 \tou\n",
      "149 \tnão\n",
      "150 \tn\n",
      "151 \timporta\n",
      "152 \tuma\n",
      "153 \tsi\n",
      "154 \tatualização\n",
      "155 \tos\n",
      "156 \tmeninos\n",
      "157 \tforam\n",
      "158 \tà\n",
      "159 \tum\n",
      "160 \tevento\n",
      "161 \tpara\n",
      "162 \tse\n",
      "163 \tencontrar\n",
      "164 \tcom\n",
      "165 \tfãs\n",
      "166 \tna\n",
      "167 \ttailândia\n",
      "168 \tdia\n",
      "169 \t01/07\n",
      "170 \tde\n",
      "171 \tacordo\n",
      "172 \tcom\n",
      "173 \ta\n",
      "174 \tdata\n",
      "175 \tocid\n",
      "176 \tdisse\n",
      "177 \ttudo\n",
      "178 \teu\n",
      "179 \todeio\n",
      "180 \tquando\n",
      "181 \ta\n",
      "182 \tgente\n",
      "183 \tpede\n",
      "184 \tpra\n",
      "185 \teu\n",
      "186 \tfazer\n",
      "187 \tum\n",
      "188 \tfavor\n",
      "189 \tpow\n",
      "190 \todeio\n",
      "191 \todeio\n",
      "192 \taté\n",
      "193 \ta\n",
      "194 \tmais\n",
      "195 \truim\n",
      "196 \tdas\n",
      "197 \tpizzas\n",
      "198 \té\n",
      "199 \tkkk\n",
      "200 \tfico\n",
      "201 \taté\n",
      "202 \tcom\n",
      "203 \tvergonha\n",
      "204 \talheia\n",
      "205 \tquando\n",
      "206 \tele\n",
      "207 \tta\n",
      "208 \tno\n",
      "209 \tchão\n",
      "210 \tfazendo\n",
      "211 \tescandalo\n",
      "212 \tminhas\n",
      "213 \tférias\n",
      "214 \tserão\n",
      "215 \tentediantes\n",
      "216 \tnamoral\n",
      "217 \tse\n",
      "218 \to\n",
      "219 \tbrasil\n",
      "220 \tganhar\n",
      "221 \tsexta\n",
      "222 \tos\n",
      "223 \tgringos\n",
      "224 \tvão\n",
      "225 \taté\n",
      "226 \tquerer\n",
      "227 \tdesativar\n",
      "228 \tporque\n",
      "229 \ta\n",
      "230 \tgente\n",
      "231 \tvai\n",
      "232 \tficar\n",
      "233 \tnojento\n",
      "234 \tbom\n",
      "235 \teu\n",
      "236 \tsempre\n",
      "237 \tdemonstrei\n",
      "238 \tpelos\n",
      "239 \toutros\n",
      "240 \taté\n",
      "241 \tnas\n",
      "242 \tsuas\n",
      "243 \tpequenas\n",
      "244 \tconquistas\n",
      "245 \tjá\n",
      "246 \teu\n",
      "247 \tprefiro\n",
      "248 \tnem\n",
      "249 \tcomentar\n",
      "250 \teles\n",
      "251 \tfalam\n",
      "252 \tque\n",
      "253 \tnão\n",
      "254 \tné\n",
      "255 \tmas\n",
      "256 \tcada\n",
      "257 \tum\n",
      "258 \ttem\n",
      "259 \tuma\n",
      "260 \tresposta\n",
      "261 \tdiferente\n",
      "262 \tquando\n",
      "263 \tperguntam\n",
      "264 \tsobre\n",
      "265 \tmcfly\n",
      "266 \tuns\n",
      "267 \tfic\n",
      "268 \tum\n",
      "269 \tano\n",
      "270 \tatrás\n",
      "271 \teu\n",
      "272 \tnunca\n",
      "273 \timaginei\n",
      "274 \tque\n",
      "275 \tminha\n",
      "276 \tvida\n",
      "277 \tseria\n",
      "278 \tcomo\n",
      "279 \té\n",
      "280 \tagora\n",
      "281 \tfoda\n",
      "282 \tde\n",
      "283 \tbeijar\n",
      "284 \tpor\n",
      "285 \tbeijar\n",
      "286 \té\n",
      "287 \tq\n",
      "288 \tsempre\n",
      "289 \tvai\n",
      "290 \tter\n",
      "291 \taquela\n",
      "292 \tespecífica\n",
      "293 \tq\n",
      "294 \tvc\n",
      "295 \tqueria\n",
      "296 \tpra\n",
      "297 \tporra\n",
      "298 \tss\n",
      "299 \tvou\n",
      "300 \tpensar\n",
      "301 \teu\n",
      "302 \tainda\n",
      "303 \tnão\n",
      "304 \tassisti\n",
      "305 \ta\n",
      "306 \tterra\n",
      "307 \tprometida\n",
      "308 \té\n",
      "309 \ta\n",
      "310 \tminha\n",
      "311 \tprimeira\n",
      "312 \tvez\n",
      "313 \ttô\n",
      "314 \tpensando\n",
      "315 \tem\n",
      "316 \tfaz\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for line in data:\n",
    "    splitted_line = line.split(' ')\n",
    "    #print splitted_line\n",
    "    sentence = []\n",
    "    for word in splitted_line:\n",
    "        if (len(word) > 0):\n",
    "            \n",
    "            word = word.lower()\n",
    "            #print word\n",
    "            word = re.sub(r'[_]','',word)\n",
    "            word = re.sub(r'@[A-Za-z0-9]+','',word)\n",
    "            word = re.sub('https?://[A-Za-z0-9./]+','',word)\n",
    "            word = re.sub(r'[\":,.!?]','',word)\n",
    "            word = re.sub(r'\\n','',word)\n",
    "            word = re.sub(r'\\[','',word)\n",
    "            word = re.sub(r'\\]','',word)\n",
    "            word = re.sub(u'\\-','',word)\n",
    "            word = re.sub(u'\\u2026','',word)\n",
    "            word = re.sub(r'\\#[A-Za-z0-9]+','',word)\n",
    "            #print('-----')\n",
    "            if word != 'rt' and word != ':' and word != '':\n",
    "                i += 1\n",
    "                print i, '\\t',word\n",
    "                sentence.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'ss', u'vou', u'pensar', u'eu', u'ainda', u'n\\xe3o', u'assisti', u'a', u'terra', u'prometida', u'\\xe9', u'a', u'minha', u'primeira', u'vez', u't\\xf4', u'pensando', u'em', u'faz']\n"
     ]
    }
   ],
   "source": [
    "print sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sentences():\n",
    "    def _init_ (self):\n",
    "        pass\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for path in files:\n",
    "            file_opened = open(path)\n",
    "            \n",
    "            for line in file_opened:\n",
    "                \n",
    "                \n",
    "                splited = line.split(\" \")\n",
    "                \n",
    "                if len(splited)>10:\n",
    "                    sentence = []\n",
    "                    for part in splited:\n",
    "                        \n",
    "                        if len(part)>0:\n",
    "                        \n",
    "                            if part!=\".\\n\" and part != \".\" and part != \"?\" and part != \"!\" and part != \"...\" and part != \" .\":\n",
    "                                if part != \"\\\"\" and part != \",\":\n",
    "                                    sentence.append(part.lower())\n",
    "                            \n",
    "                            else:\n",
    "                                yield sentence\n",
    "                                sentence = []  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-07-04 00:49:04,935 : INFO : collecting all words and their counts\n"
     ]
    },
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: u'@'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-2a7a6d02f6d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWord2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmin_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwindow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/dranaju/.local/lib/python2.7/site-packages/gensim/models/word2vec.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sentences, size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, cbow_mean, hashfxn, iter, null_word, trim_rule, sorted_vocab, batch_words, compute_loss, callbacks)\u001b[0m\n\u001b[1;32m    525\u001b[0m             \u001b[0mbatch_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m             \u001b[0mhs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnegative\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcbow_mean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcbow_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmin_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 527\u001b[0;31m             fast_version=FAST_VERSION)\n\u001b[0m\u001b[1;32m    528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_do_train_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/dranaju/.local/lib/python2.7/site-packages/gensim/models/base_any2vec.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sentences, workers, vector_size, epochs, callbacks, batch_words, trim_rule, sg, alpha, window, seed, hs, negative, cbow_mean, min_alpha, compute_loss, fast_version, **kwargs)\u001b[0m\n\u001b[1;32m    333\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGeneratorType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You can't pass a generator as the sentences argument. Try an iterator.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m             self.train(\n\u001b[1;32m    337\u001b[0m                 \u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/dranaju/.local/lib/python2.7/site-packages/gensim/models/base_any2vec.pyc\u001b[0m in \u001b[0;36mbuild_vocab\u001b[0;34m(self, sentences, update, progress_per, keep_raw_vocab, trim_rule, **kwargs)\u001b[0m\n\u001b[1;32m    478\u001b[0m         \"\"\"\n\u001b[1;32m    479\u001b[0m         total_words, corpus_count = self.vocabulary.scan_vocab(\n\u001b[0;32m--> 480\u001b[0;31m             sentences, progress_per=progress_per, trim_rule=trim_rule)\n\u001b[0m\u001b[1;32m    481\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpus_count\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m         report_values = self.vocabulary.prepare_vocab(\n",
      "\u001b[0;32m/home/dranaju/.local/lib/python2.7/site-packages/gensim/models/word2vec.pyc\u001b[0m in \u001b[0;36mscan_vocab\u001b[0;34m(self, sentences, progress_per, trim_rule)\u001b[0m\n\u001b[1;32m   1149\u001b[0m         \u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m         \u001b[0mchecked_string_types\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1151\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0msentence_no\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1152\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mchecked_string_types\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-60-912cb8824bd7>\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m             \u001b[0mfile_opened\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfile_opened\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: u'@'"
     ]
    }
   ],
   "source": [
    "model = gensim.models.Word2Vec(sentences(),size=300,min_count=10, workers=10,window = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
